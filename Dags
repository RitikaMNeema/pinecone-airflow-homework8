from airflow import DAG
from datetime import datetime, timedelta
from airflow.decorators import task
from airflow.models import Variable
import pandas as pd
import time
import requests
import os
import json

from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='Medium_to_Pinecone',
    default_args=default_args,
    description='Build a Medium Posting Search Engine using Pinecone',
    schedule_interval=None,  # run manually for screenshots; set timedelta(days=7) if you want weekly
    start_date=datetime(2025, 4, 1),
    catchup=False,
    tags=['medium', 'pinecone', 'search-engine'],
) as dag:

    @task
    def download_data():
        data_dir = '/opt/airflow/include/medium_data'
        os.makedirs(data_dir, exist_ok=True)
        file_path = f"{data_dir}/medium_data.csv"

        url = 'https://s3-geospatial.s3.us-west-2.amazonaws.com/medium_data.csv'
        r = requests.get(url, stream=True, timeout=60)
        if r.status_code != 200:
            raise Exception(f"Failed to download data: HTTP {r.status_code}")

        with open(file_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            line_count = sum(1 for _ in f)
        print(f"Downloaded file has {line_count} lines")

        return file_path

    @task
    def preprocess_data(data_path: str):
        df = pd.read_csv(data_path)

        # Clean string fields (avoid 'nan' strings)
        for col in ['title', 'subtitle']:
            if col in df.columns:
                df[col] = df[col].fillna('').astype(str)
            else:
                df[col] = ''

        # Text to embed = title + subtitle
        df['text'] = (df['title'] + ' ' + df['subtitle']).str.strip()

        # Simple string IDs
        df = df.reset_index(drop=True)
        df['id'] = df.index.astype(str)

        # Optional metadata (JSON-safe)
        df['metadata'] = df.apply(
            lambda r: json.dumps({'title': r['title'], 'subtitle': r['subtitle']}),
            axis=1
        )

        out_path = '/opt/airflow/include/medium_data/medium_preprocessed.csv'
        df[['id', 'text', 'metadata']].to_csv(out_path, index=False)
        print(f"Preprocessed data saved to {out_path}")
        return out_path

    @task
    def create_pinecone_index():
        api_key = Variable.get("PINECONE_API_KEY")  # <-- use the uppercase key
        region = Variable.get("PINECONE_REGION", default_var="us-east-1")
        cloud = Variable.get("PINECONE_CLOUD", default_var="aws")
        index_name = Variable.get("PINECONE_INDEX_NAME", default_var="semantic-search-fast")

        pc = Pinecone(api_key=api_key)

        # Create serverless index if missing
        existing = [i["name"] for i in pc.list_indexes()]
        if index_name not in existing:
            print(f"Creating index '{index_name}'...")
            pc.create_index(
                name=index_name,
                dimension=384,            # all-MiniLM-L6-v2
                metric="cosine",
                spec=ServerlessSpec(cloud=cloud, region=region),
            )

        # Wait until ready
        while True:
            desc = pc.describe_index(index_name)
            if getattr(desc.status, "ready", False):
                break
            print("Waiting for index to be ready...")
            time.sleep(2)

        print(f"Pinecone index '{index_name}' is ready")
        return index_name

    @task
    def generate_embeddings_and_upsert(preprocessed_csv: str, index_name: str):
        api_key = Variable.get("PINECONE_API_KEY")
        pc = Pinecone(api_key=api_key)
        index = pc.Index(index_name)

        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        df = pd.read_csv(preprocessed_csv)

        batch_size = 100
        total = 0

        for start in range(0, len(df), batch_size):
            batch = df.iloc[start:start + batch_size].copy()
            texts = batch['text'].tolist()

            # Normalize for cosine
            embs = model.encode(texts, normalize_embeddings=True, show_progress_bar=False)

            # Parse metadata JSON
            metas = [json.loads(m) if isinstance(m, str) else (m or {}) for m in batch['metadata']]

            # Prepare vectors
            vectors = []
            for (i, row), e, meta in zip(batch.iterrows(), embs, metas):
                vectors.append({
                    "id": str(row['id']),
                    "values": e.tolist(),
                    "metadata": meta
                })

            index.upsert(vectors=vectors)
            total += len(vectors)
            print(f"Upserted {total}/{len(df)} vectors...")

        print(f"Successfully upserted {total} records to Pinecone")
        return index_name

    @task
    def test_search_query(index_name: str):
        api_key = Variable.get("PINECONE_API_KEY")
        pc = Pinecone(api_key=api_key)
        index = pc.Index(index_name)

        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

        query = "what is ethics in AI"
        q = model.encode([query], normalize_embeddings=True, show_progress_bar=False)[0].tolist()

        res = index.query(vector=q, top_k=5, include_metadata=True)
        print(f"Search results for query: '{query}'")
        for m in res.get('matches', []):
            meta = m.get('metadata', {}) or {}
            title = meta.get('title', '')[:80]
            print(f"ID: {m.get('id')}  Score: {m.get('score'):.4f}  Title: {title}...")

    data_path = download_data()
    preprocessed_path = preprocess_data(data_path)
    index_name = create_pinecone_index()
    final_index_name = generate_embeddings_and_upsert(preprocessed_path, index_name)
    test_search_query(final_index_name)
